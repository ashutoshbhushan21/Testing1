import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import pickle

# Configuration
base_url = "http://www.tscs.example.com"  # Change to the actual base URL
allowed_domains = ["tscs.example.com"]  # List of allowed domains
homepages = ["http://www.tscs.example.com/home1", "http://www.tscs.example.com/home2"]  # Start URLs

# Data Structures
visited = set()
graph = {}
url_queue = deque(homepages)  # Queue of URLs to visit, initialized with homepages

# Function to determine if the URL is within the allowed scope
def is_within_scope(url):
    netloc = urlparse(url).netloc
    if netloc not in allowed_domains:
        return False
    if "intranet" in url:  # Simple check to filter out intranet links
        return False
    return True

# Function to process pages in a breadth-first manner
def crawl():
    while url_queue:
        url = url_queue.popleft()
        if url not in visited and is_within_scope(url):
            try:
                response = requests.get(url)
                if response.status_code == 200:
                    visited.add(url)
                    page_content = response.text
                    soup = BeautifulSoup(page_content, 'html.parser')

                    # Extract metadata like title
                    title = soup.find('title').string if soup.find('title') else ''
                    
                    # Add to graph
                    graph[url] = {
                        'title': title,
                        'links': []
                    }

                    # Find and process all links on the page
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        graph[url]['links'].append(full_url)
                        if full_url not in visited:
                            url_queue.append(full_url)

            except requests.exceptions.RequestException as e:
                print(f"Request failed: {e}")

# Start crawling
crawl()

# Save the graph using pickle for later analysis
with open('web_graph.pkl', 'wb') as f:
    pickle.dump(graph, f)




---------------------------




import requests
from bs4 import BeautifulSoup
import networkx as nx
import pickle
from urllib.parse import urljoin, urlparse

# Initialize a directed graph
graph = nx.DiGraph()

# Initialize a set to keep track of visited URLs
visited = set()

# The BFS queue
queue = []

def get_links(url, domain):
    """Return the links found in the given webpage within the same domain."""
    links = []
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a', href=True):
                absolute_link = urljoin(url, link['href'])
                if urlparse(absolute_link).netloc == domain:
                    links.append(absolute_link)
    except requests.RequestException:
        pass
    return links

def save_graph(graph):
    """Save the graph object to a file."""
    with open('web_graph.pkl', 'wb') as f:
        pickle.dump(graph, f)

def crawl(seed_url):
    """Crawl the web starting from the seed_url."""
    domain = urlparse(seed_url).netloc
    queue.append(seed_url)

    while queue:
        current_url = queue.pop(0)
        if current_url not in visited:
            visited.add(current_url)
            print(f"Crawling: {current_url}")
            for link in get_links(current_url, domain):
                if link not in visited:  # Check if not visited to prevent loops
                    graph.add_edge(current_url, link)
                    queue.append(link)
            save_graph(graph)  # Save after each URL is processed

# Set the seed URL
seed = 'http://example.com'

# Start crawling
crawl(seed)

# Once finished, you can load the graph like this:
with open('web_graph.pkl', 'rb') as f:
    graph = pickle.load(f)

# To visualize the graph, you might use NetworkX's drawing capabilities or integrate with a visualization library like Pyvis.
import matplotlib.pyplot as plt
nx.draw(graph, with_labels=True)
plt.show()



--------------------------





import requests
from bs4 import BeautifulSoup
import networkx as nx
import pickle
from urllib.parse import urljoin, urlparse

# Initialize a directed graph
graph = nx.DiGraph()

# Initialize a set to keep track of visited URLs
visited = set()

# The BFS queue
queue = []

def get_links(url, domain):
    """Return the links found in the given webpage within the same domain."""
    links = []
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            for link in soup.find_all('a', href=True):
                absolute_link = urljoin(url, link['href'])
                if urlparse(absolute_link).netloc == domain:
                    links.append(absolute_link)
    except requests.RequestException:
        pass
    return links

def save_graph(graph):
    """Save the graph object to a file."""
    with open('web_graph.pkl', 'wb') as f:
        pickle.dump(graph, f)

def crawl(seed_url, max_depth):
    """Crawl the web starting from the seed_url."""
    domain = urlparse(seed_url).netloc
    queue.append((seed_url, 0))

    while queue:
        current_url, current_depth = queue.pop(0)
        if current_url not in visited and current_depth <= max_depth:
            visited.add(current_url)
            print(f"Crawling: {current_url}")
            for link in get_links(current_url, domain):
                graph.add_edge(current_url, link)
                queue.append((link, current_depth + 1))
            save_graph(graph)  # Save after each URL is processed

# Set the seed URL and max depth
seed = 'http://example.com'
max_depth = 2

# Start crawling
crawl(seed, max_depth)

# Once finished, you can load the graph like this:
with open('web_graph.pkl', 'rb') as f:
    graph = pickle.load(f)

# To visualize the graph, you might use NetworkX's drawing capabilities or integrate with a visualization library like Pyvis.
import matplotlib.pyplot as plt
nx.draw(graph, with_labels=True)
plt.show()



------------------------------------------------------



import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import networkx as nx
import pickle

# Configuration
base_url = "http://www.tscs.example.com"  # Change to the actual base URL
allowed_domains = ["tscs.example.com"]  # List of allowed domains
homepages = ["http://www.tscs.example.com/home1", "http://www.tscs.example.com/home2"]  # Start URLs

# Data Structures
visited = set()
graph = nx.DiGraph()  # Initialize a directed graph using networkx
url_queue = deque(homepages)  # Queue of URLs to visit, initialized with homepages

# Function to determine if the URL is within the allowed scope
def is_within_scope(url):
    netloc = urlparse(url).netloc
    if netloc not in allowed_domains:
        return False
    if "intranet" in url:  # Simple check to filter out intranet links
        return False
    return True

# Function to process pages in a breadth-first manner
def crawl():
    while url_queue:
        url = url_queue.popleft()
        if url not in visited and is_within_scope(url):
            try:
                response = requests.get(url)
                if response.status_code == 200:
                    visited.add(url)
                    page_content = response.text
                    soup = BeautifulSoup(page_content, 'html.parser')

                    # Extract metadata like title
                    title = soup.find('title').string if soup.find('title') else ''

                    # Add the URL and its title as a node in the graph
                    graph.add_node(url, title=title)

                    # Find and process all links on the page
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        if is_within_scope(full_url):
                            # Add an edge in the graph from this page to the linked page
                            graph.add_edge(url, full_url)
                            if full_url not in visited:
                                url_queue.append(full_url)

            except requests.exceptions.RequestException as e:
                print(f"Request failed: {e}")

# Start crawling
crawl()

# Save the graph using pickle for later analysis
with open('web_graph.pkl', 'wb') as f:
    pickle.dump(graph, f)

# Optionally, print the graph (or part of it) to the console for quick inspection
print("Nodes in the graph:")
for node in graph.nodes(data=True):
    print(node)

print("\nEdges in the graph:")
for edge in graph.edges():
    print(edge)
---------------------------------------





import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import networkx as nx
from collections import deque

# Define the set of seed URLs and allowed domain for confined crawling
seed_urls = ["http://example-section1.com", "http://example-section2.com"]
allowed_domains = ["example-section1.com", "example-section2.com"]
file_types = ['.pdf', '.doc', '.docx', '.ppt', '.pptx']

# Initialize a directed graph
G = nx.DiGraph()

# A set to keep track of visited nodes
visited = set(seed_urls)  # Seed URLs are initially marked as visited

# A queue to keep track of URLs to be crawled
queue = deque(seed_urls)

# Function to check if a URL is a file of interest and track it
def track_file(url):
    if any(url.endswith(ext) for ext in file_types):
        # Add a node for the file with a tag indicating it's a file
        G.add_node(url, file=True)

# Function to perform the crawling
def crawl():
    while queue:
        current_url = queue.popleft()

        # Check if the current URL is a file and if so, track it
        if any(current_url.endswith(ext) for ext in file_types):
            track_file(current_url)
            continue

        try:
            response = requests.get(current_url, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")
                G.add_node(current_url, title=soup.title.string if soup.title else 'No Title', file=False)

                for link in soup.find_all('a', href=True):
                    absolute_link = urljoin(current_url, link['href'])
                    parsed_link = urlparse(absolute_link)
                    clean_link = f"{parsed_link.scheme}://{parsed_link.netloc}{parsed_link.path}"

                    if parsed_link.netloc in allowed_domains and clean_link not in visited:
                        visited.add(clean_link)
                        G.add_edge(current_url, clean_link)
                        queue.append(clean_link)

                        # Check and track if the link is a downloadable file
                        if any(clean_link.endswith(ext) for ext in file_types):
                            track_file(clean_link)
                        
                        # Save intermediate results
                        nx.write_gpickle(G, "web_crawl.gpickle")

        except requests.exceptions.RequestException as e:
            print(f"Request failed for {current_url}: {e}")
            G.add_node(current_url, error=str(e), file=False)  # Add error information to the graph

# Start the crawling process
crawl()

# To read the graph back after stopping the script
# G = nx.read_gpickle("web_crawl.gpickle")



--------------------------------------------




import requests
from bs4 import BeautifulSoup
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import time
import networkx as nx
import pickle

# Initialize a directed graph
graph = nx.DiGraph()

# Set of visited URLs
visited = set()

# Queue of URLs to be crawled
queue = []

# RobotFileParser to respect robots.txt
rp = RobotFileParser()

def can_fetch(url):
    """Check if the robots.txt allows the crawling of the url."""
    rp.set_url(urljoin(url, '/robots.txt'))
    rp.read()
    user_agent = '*'
    return rp.can_fetch(user_agent, url)

def get_links(url):
    """Return the links found within the same domain of the given URL."""
    domain = urlparse(url).netloc
    links = []
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            content_type = response.headers.get('Content-Type')
            if content_type and 'html' in content_type.lower():
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    absolute_link = urljoin(url, link['href'])
                    if urlparse(absolute_link).netloc == domain:
                        links.append(absolute_link)
    except requests.RequestException as e:
        print(f"Request failed for {url}: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")
    return links

def save_graph(graph):
    """Save the graph object to a file."""
    with open('web_graph.pkl', 'wb') as f:
        pickle.dump(graph, f)

def crawl(seed_url, max_pages=100):
    """Crawl the web starting from the seed_url."""
    queue.append(seed_url)

    while queue and len(visited) < max_pages:
        current_url = queue.pop(0)
        if current_url not in visited and can_fetch(current_url):
            print(f"Crawling: {current_url}")
            visited.add(current_url)
            for link in get_links(current_url):
                if link not in visited:
                    graph.add_edge(current_url, link)
                    queue.append(link)
            save_graph(graph)  # Save after each URL is processed
            time.sleep(1)  # Sleep to respect server load

# Set the seed URL
seed = 'http://example.com'

# Start crawling
crawl(seed)

# Load the graph for visualization
with open('web_graph.pkl', 'rb') as f:
    graph = pickle.load(f)

# Visualization can be done using networkx draw or other sophisticated libraries such as Pyvis
import matplotlib.pyplot as plt
nx.draw(graph, with_labels=True)
plt.show()

